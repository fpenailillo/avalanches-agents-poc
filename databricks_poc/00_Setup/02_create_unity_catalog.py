# Databricks notebook source
# MAGIC %md
# MAGIC # Creaci√≥n de Unity Catalog - POC Avalanche Prediction
# MAGIC
# MAGIC **Ejecutar UNA VEZ** para crear la estructura completa de Unity Catalog:
# MAGIC - Cat√°logo
# MAGIC - Schema
# MAGIC - Volumes
# MAGIC - Tablas base (estructura vac√≠a)
# MAGIC
# MAGIC **IMPORTANTE:** Requiere permisos de ADMIN en Databricks

# COMMAND ----------

# MAGIC %run ./00_environment_setup

# COMMAND ----------

# MAGIC %md
# MAGIC ## 1. Crear Cat√°logo

# COMMAND ----------

print(f"üì¶ Creando cat√°logo: {CATALOG}")

try:
    spark.sql(f"CREATE CATALOG IF NOT EXISTS {CATALOG}")
    print(f"‚úÖ Cat√°logo '{CATALOG}' creado/verificado")
except Exception as e:
    print(f"‚ö†Ô∏è  Error creando cat√°logo: {e}")
    print("   ‚Üí Puede que ya exista o que necesites permisos de ADMIN")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 2. Crear Schema

# COMMAND ----------

print(f"üìä Creando schema: {SCHEMA}")

try:
    spark.sql(f"CREATE SCHEMA IF NOT EXISTS {FULL_DATABASE}")
    spark.sql(f"USE {FULL_DATABASE}")
    print(f"‚úÖ Schema '{FULL_DATABASE}' creado/verificado")
except Exception as e:
    print(f"‚ö†Ô∏è  Error creando schema: {e}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 3. Crear Volume para Archivos Raw

# COMMAND ----------

print(f"üíæ Creando volume: {VOLUME}")

try:
    spark.sql(f"""
        CREATE VOLUME IF NOT EXISTS {FULL_DATABASE}.{VOLUME}
    """)
    print(f"‚úÖ Volume '{VOLUME}' creado/verificado")
    print(f"   Path: {VOLUME_PATH}")
except Exception as e:
    print(f"‚ö†Ô∏è  Error creando volume: {e}")
    print("   ‚Üí Volumes solo disponibles en Unity Catalog habilitado")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 4. Crear Directorios en Volume

# COMMAND ----------

print("üìÅ Creando estructura de directorios en volume...")

directories = [
    f"{VOLUME_PATH}/raw",
    f"{VOLUME_PATH}/raw/srtm",
    f"{VOLUME_PATH}/raw/weather",
    f"{VOLUME_PATH}/raw/relatos",
    f"{VOLUME_PATH}/processed",
    f"{VOLUME_PATH}/processed/topo",
    f"{VOLUME_PATH}/processed/nlp",
    f"{VOLUME_PATH}/processed/weather",
    f"{VOLUME_PATH}/models",
    f"{VOLUME_PATH}/models/nlp",
    f"{VOLUME_PATH}/outputs",
    f"{VOLUME_PATH}/outputs/maps",
    f"{VOLUME_PATH}/outputs/boletines"
]

for directory in directories:
    try:
        dbutils.fs.mkdirs(directory)
        print(f"   ‚úÖ {directory}")
    except Exception as e:
        print(f"   ‚ö†Ô∏è  {directory}: {e}")

print("‚úÖ Estructura de directorios creada")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 5. Tabla: Ubicaciones de Centros de Esqu√≠

# COMMAND ----------

print(f"üìç Creando tabla: {TABLE_LOCATIONS}")

spark.sql(f"""
CREATE TABLE IF NOT EXISTS {TABLE_LOCATIONS} (
    location_id STRING,
    name STRING,
    latitude DOUBLE,
    longitude DOUBLE,
    elevation INT,
    region STRING,
    country STRING,
    timezone STRING,
    created_at TIMESTAMP
)
USING DELTA
COMMENT 'Ubicaciones de centros de esqu√≠ y zonas de monitoreo'
""")

print(f"‚úÖ Tabla '{TABLE_LOCATIONS}' creada")

# COMMAND ----------

# Insertar La Parva como ubicaci√≥n piloto
spark.sql(f"""
INSERT INTO {TABLE_LOCATIONS} VALUES (
    'la_parva_001',
    'La Parva',
    -33.35,
    -70.27,
    3000,
    'Regi√≥n Metropolitana',
    'Chile',
    'America/Santiago',
    current_timestamp()
)
""")

print("‚úÖ Ubicaci√≥n piloto 'La Parva' insertada")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 6. Tablas Bronze Layer (Raw Data)

# COMMAND ----------

print("ü•â Creando tablas Bronze Layer (Raw Data)...\n")

# === Tabla: Weather Daily ===
print(f"   Creando: {TABLE_WEATHER_DAILY}")
spark.sql(f"""
CREATE TABLE IF NOT EXISTS {TABLE_WEATHER_DAILY} (
    location_name STRING,
    latitude DOUBLE,
    longitude DOUBLE,
    elevation INT,
    date DATE,
    temperature_2m_max DOUBLE,
    temperature_2m_min DOUBLE,
    temperature_2m_mean DOUBLE,
    precipitation_sum DOUBLE,
    precipitation_hours DOUBLE,
    precipitation_probability_max DOUBLE,
    snowfall_sum DOUBLE,
    windspeed_10m_max DOUBLE,
    windgusts_10m_max DOUBLE,
    winddirection_10m_dominant DOUBLE,
    shortwave_radiation_sum DOUBLE,
    et0_fao_evapotranspiration DOUBLE,
    ingestion_timestamp TIMESTAMP
)
USING DELTA
PARTITIONED BY (location_name, date)
COMMENT 'Datos meteorol√≥gicos diarios de Open-Meteo'
""")

# === Tabla: Weather Hourly ===
print(f"   Creando: {TABLE_WEATHER_HOURLY}")
spark.sql(f"""
CREATE TABLE IF NOT EXISTS {TABLE_WEATHER_HOURLY} (
    location_name STRING,
    latitude DOUBLE,
    longitude DOUBLE,
    elevation INT,
    timestamp TIMESTAMP,
    date DATE,
    temperature_2m DOUBLE,
    precipitation DOUBLE,
    snowfall DOUBLE,
    snow_depth DOUBLE,
    weathercode INT,
    cloudcover INT,
    windspeed_10m DOUBLE,
    winddirection_10m DOUBLE,
    windgusts_10m DOUBLE,
    surface_pressure DOUBLE,
    relativehumidity_2m INT,
    ingestion_timestamp TIMESTAMP
)
USING DELTA
PARTITIONED BY (location_name, date)
COMMENT 'Datos meteorol√≥gicos horarios de Open-Meteo'
""")

# === Tabla: SRTM Raw ===
print(f"   Creando: {TABLE_SRTM_RAW}")
spark.sql(f"""
CREATE TABLE IF NOT EXISTS {TABLE_SRTM_RAW} (
    pixel_id STRING,
    latitude DOUBLE,
    longitude DOUBLE,
    elevation INT,
    zone_name STRING,
    ingestion_timestamp TIMESTAMP
)
USING DELTA
COMMENT 'Datos raw de elevaci√≥n SRTM para zona piloto'
""")

print("\n‚úÖ Tablas Bronze Layer creadas")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 7. Tablas Silver Layer (Processed Features)

# COMMAND ----------

print("ü•à Creando tablas Silver Layer (Processed Features)...\n")

# === Tabla: Topographic Features ===
print(f"   Creando: {TABLE_TOPO_FEATURES}")
spark.sql(f"""
CREATE TABLE IF NOT EXISTS {TABLE_TOPO_FEATURES} (
    feature_id STRING,
    latitude DOUBLE,
    longitude DOUBLE,
    elevation INT,
    slope_degrees DOUBLE,
    aspect_degrees DOUBLE,
    elevation_band STRING,
    is_critical_slope BOOLEAN,
    zone_name STRING,
    processing_timestamp TIMESTAMP
)
USING DELTA
COMMENT 'Features topogr√°ficos procesados (pendiente, orientaci√≥n, banda altitudinal)'
""")

# === Tabla: Topographic Susceptibility ===
print(f"   Creando: {TABLE_TOPO_SUSCEPTIBILITY}")
spark.sql(f"""
CREATE TABLE IF NOT EXISTS {TABLE_TOPO_SUSCEPTIBILITY} (
    zone_id STRING,
    zone_name STRING,
    elevation_band STRING,
    total_area_km2 DOUBLE,
    critical_slope_area_km2 DOUBLE,
    susceptibility_score DOUBLE,
    dominant_aspect STRING,
    avg_slope_degrees DOUBLE,
    processing_timestamp TIMESTAMP
)
USING DELTA
COMMENT 'Mapa de susceptibilidad topogr√°fica por zona y banda altitudinal'
""")

# === Tabla: NLP Risk Patterns ===
print(f"   Creando: {TABLE_NLP_PATTERNS}")
spark.sql(f"""
CREATE TABLE IF NOT EXISTS {TABLE_NLP_PATTERNS} (
    pattern_id STRING,
    zone_name STRING,
    latitude DOUBLE,
    longitude DOUBLE,
    elevation INT,
    risk_mentions INT,
    avalanche_events INT,
    difficulty_level STRING,
    sentiment STRING,
    confidence DOUBLE,
    source STRING,
    processing_timestamp TIMESTAMP
)
USING DELTA
COMMENT 'Patrones de riesgo extra√≠dos de relatos NLP'
""")

# === Tabla: Weather Features ===
print(f"   Creando: {TABLE_WEATHER_FEATURES}")
spark.sql(f"""
CREATE TABLE IF NOT EXISTS {TABLE_WEATHER_FEATURES} (
    feature_id STRING,
    location_name STRING,
    date DATE,
    elevation_band STRING,
    snowfall_24h DOUBLE,
    snowfall_48h DOUBLE,
    snowfall_72h DOUBLE,
    temp_variation_24h DOUBLE,
    max_wind_speed DOUBLE,
    precipitation_intensity STRING,
    isoterma_0c INT,
    processing_timestamp TIMESTAMP
)
USING DELTA
PARTITIONED BY (location_name, date)
COMMENT 'Features meteorol√≥gicos agregados por ventanas temporales'
""")

# === Tabla: Weather Triggers ===
print(f"   Creando: {TABLE_WEATHER_TRIGGERS}")
spark.sql(f"""
CREATE TABLE IF NOT EXISTS {TABLE_WEATHER_TRIGGERS} (
    trigger_id STRING,
    location_name STRING,
    date DATE,
    elevation_band STRING,
    trigger_type STRING,
    trigger_severity STRING,
    snowfall_cm DOUBLE,
    temp_change_c DOUBLE,
    wind_kmh DOUBLE,
    is_critical BOOLEAN,
    processing_timestamp TIMESTAMP
)
USING DELTA
PARTITIONED BY (location_name, date)
COMMENT 'Detecci√≥n de condiciones gatillantes de avalanchas'
""")

print("\n‚úÖ Tablas Silver Layer creadas")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 8. Tablas Gold Layer (Analytics & Predictions)

# COMMAND ----------

print("ü•á Creando tablas Gold Layer (Analytics & Predictions)...\n")

# === Tabla: Avalanche Predictions ===
print(f"   Creando: {TABLE_RISK_PREDICTIONS}")
spark.sql(f"""
CREATE TABLE IF NOT EXISTS {TABLE_RISK_PREDICTIONS} (
    prediction_id STRING,
    zone_name STRING,
    forecast_date DATE,
    elevation_band STRING,
    eaws_level INT,
    eaws_label STRING,
    risk_score DOUBLE,
    confidence DOUBLE,
    topo_contribution DOUBLE,
    weather_contribution DOUBLE,
    nlp_contribution DOUBLE,
    main_factors ARRAY<STRING>,
    prediction_timestamp TIMESTAMP
)
USING DELTA
PARTITIONED BY (zone_name, forecast_date)
COMMENT 'Predicciones de riesgo de avalanchas por zona y banda altitudinal'
""")

# === Tabla: Avalanche Bulletins ===
print(f"   Creando: {TABLE_BOLETINES}")
spark.sql(f"""
CREATE TABLE IF NOT EXISTS {TABLE_BOLETINES} (
    boletin_id STRING,
    zone_name STRING,
    issue_date DATE,
    valid_from DATE,
    valid_to DATE,
    eaws_level INT,
    eaws_label STRING,
    summary STRING,
    danger_description STRING,
    avalanche_problems ARRAY<STRING>,
    affected_elevations STRING,
    affected_aspects STRING,
    recommendations STRING,
    confidence STRING,
    bulletin_text STRING,
    generated_by STRING,
    creation_timestamp TIMESTAMP
)
USING DELTA
PARTITIONED BY (zone_name, issue_date)
COMMENT 'Boletines de avalanchas generados autom√°ticamente'
""")

# === Tabla: Susceptibility Map ===
print(f"   Creando: {TABLE_SUSCEPTIBILITY_MAP}")
spark.sql(f"""
CREATE TABLE IF NOT EXISTS {TABLE_SUSCEPTIBILITY_MAP} (
    grid_id STRING,
    zone_name STRING,
    latitude DOUBLE,
    longitude DOUBLE,
    elevation INT,
    elevation_band STRING,
    slope_degrees DOUBLE,
    aspect_degrees DOUBLE,
    susceptibility_score DOUBLE,
    risk_category STRING,
    last_updated TIMESTAMP
)
USING DELTA
COMMENT 'Mapa de susceptibilidad de alta resoluci√≥n para visualizaci√≥n'
""")

print("\n‚úÖ Tablas Gold Layer creadas")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 9. Verificaci√≥n de Tablas Creadas

# COMMAND ----------

print("üîç Verificando tablas creadas...\n")

tables = spark.sql(f"SHOW TABLES IN {FULL_DATABASE}").collect()

print(f"üìä Tablas en {FULL_DATABASE}:")
print("=" * 70)

bronze_count = 0
silver_count = 0
gold_count = 0

for table in tables:
    table_name = table.tableName
    print(f"   ‚úÖ {table_name}")

    # Clasificar por layer
    if any(x in table_name for x in ['weather_daily', 'weather_hourly', 'srtm_raw', 'locations']):
        bronze_count += 1
    elif any(x in table_name for x in ['topo_features', 'topo_susceptibility', 'nlp_', 'weather_features', 'weather_triggers']):
        silver_count += 1
    elif any(x in table_name for x in ['predictions', 'bulletins', 'susceptibility_map']):
        gold_count += 1

print("=" * 70)
print(f"\nüìà RESUMEN:")
print(f"   ü•â Bronze Layer: {bronze_count} tablas")
print(f"   ü•à Silver Layer: {silver_count} tablas")
print(f"   ü•á Gold Layer: {gold_count} tablas")
print(f"   üìä TOTAL: {len(tables)} tablas")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 10. Verificaci√≥n de Ubicaciones

# COMMAND ----------

print("\nüìç Verificando ubicaciones piloto:")
print("=" * 70)

locations_df = spark.table(TABLE_LOCATIONS)
display(locations_df)

print(f"\n‚úÖ {locations_df.count()} ubicaci√≥n(es) configurada(s)")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 11. Resumen Final

# COMMAND ----------

print("\n" + "=" * 70)
print("üìã RESUMEN DE UNITY CATALOG - POC AVALANCHE PREDICTION")
print("=" * 70)

print(f"""
‚úÖ CAT√ÅLOGO CREADO: {CATALOG}
‚úÖ SCHEMA CREADO: {SCHEMA}
‚úÖ DATABASE COMPLETO: {FULL_DATABASE}
‚úÖ VOLUME CREADO: {VOLUME}

üìä ESTRUCTURA DE DATOS (Medallion Architecture):

ü•â BRONZE LAYER (Raw Data):
   ‚Ä¢ {TABLE_LOCATIONS.split('.')[-1]}
   ‚Ä¢ {TABLE_WEATHER_DAILY.split('.')[-1]}
   ‚Ä¢ {TABLE_WEATHER_HOURLY.split('.')[-1]}
   ‚Ä¢ {TABLE_SRTM_RAW.split('.')[-1]}

ü•à SILVER LAYER (Processed Features):
   ‚Ä¢ {TABLE_TOPO_FEATURES.split('.')[-1]}
   ‚Ä¢ {TABLE_TOPO_SUSCEPTIBILITY.split('.')[-1]}
   ‚Ä¢ {TABLE_NLP_PATTERNS.split('.')[-1]}
   ‚Ä¢ {TABLE_WEATHER_FEATURES.split('.')[-1]}
   ‚Ä¢ {TABLE_WEATHER_TRIGGERS.split('.')[-1]}

ü•á GOLD LAYER (Analytics & Predictions):
   ‚Ä¢ {TABLE_RISK_PREDICTIONS.split('.')[-1]}
   ‚Ä¢ {TABLE_BOLETINES.split('.')[-1]}
   ‚Ä¢ {TABLE_SUSCEPTIBILITY_MAP.split('.')[-1]}

üìÅ VOLUMES:
   ‚Ä¢ {VOLUME_PATH}/raw/
   ‚Ä¢ {VOLUME_PATH}/processed/
   ‚Ä¢ {VOLUME_PATH}/models/
   ‚Ä¢ {VOLUME_PATH}/outputs/

üèîÔ∏è  ZONA PILOTO:
   ‚Ä¢ La Parva, Regi√≥n Metropolitana
   ‚Ä¢ Coordenadas: (-33.35, -70.27)
   ‚Ä¢ Elevaci√≥n: 3000m

üìù PR√ìXIMOS PASOS:
   1. Ejecutar Agente 1: Topogr√°fico (SRTM processing)
   2. Ejecutar Agente 2: NLP (relatos analysis)
   3. Ejecutar Agente 3: Meteorol√≥gico (weather ingestion)
   4. Ejecutar Agente 4: Integrador (risk fusion)
   5. Visualizar resultados en dashboard

""")

print("=" * 70)
print("üéâ UNITY CATALOG CONFIGURADO EXITOSAMENTE")
print("   ‚Üí El sistema est√° listo para ingesta de datos")
print("=" * 70)
