# Databricks notebook source
# MAGIC %md
# MAGIC # Agente 1: Mapa de Susceptibilidad Topogr√°fica - GEE v2
# MAGIC
# MAGIC **Versi√≥n refactorizada para trabajar con datos de Google Earth Engine**
# MAGIC
# MAGIC Este notebook genera el mapa de susceptibilidad basado en features topogr√°ficos
# MAGIC procesados desde Google Earth Engine.
# MAGIC
# MAGIC **Cambios en v2:**
# MAGIC - Trabaja con datos de alta calidad de GEE
# MAGIC - Considera metadatos de calidad GEE
# MAGIC - Validaciones estrictas (sin fallbacks)
# MAGIC - An√°lisis de cobertura y confiabilidad
# MAGIC
# MAGIC **Input:** `topo_features` (capa Silver con datos GEE)
# MAGIC **Output:** `topo_susceptibility` (capa Silver) y `susceptibility_map` (capa Gold)

# COMMAND ----------

# MAGIC %run ../00_Setup/00_environment_setup

# COMMAND ----------

# MAGIC %md
# MAGIC ## 1. Verificar Configuraci√≥n y Cargar Features

# COMMAND ----------

from pyspark.sql import functions as F
from pyspark.sql.window import Window
import uuid
from datetime import datetime

print("üîç VERIFICANDO CONFIGURACI√ìN...")
print(f"   Full Database: {FULL_DATABASE}")
print(f"   Tabla origen: {TABLE_TOPO_FEATURES}")
print(f"   Tabla destino susceptibility: {TABLE_TOPO_SUSCEPTIBILITY}")
print(f"   Tabla destino mapa: {TABLE_SUSCEPTIBILITY_MAP}")

# Verificar schema activo
try:
    spark.sql(f"USE {FULL_DATABASE}")
    print(f"‚úÖ Schema '{FULL_DATABASE}' est√° activo")
except Exception as e:
    print(f"‚ùå ERROR con schema: {e}")
    raise

print(f"\nüìä Cargando features topogr√°ficos desde: {TABLE_TOPO_FEATURES}")

# Verificar que la tabla existe
if not spark.catalog.tableExists(TABLE_TOPO_FEATURES):
    raise Exception(f"‚ùå ERROR CR√çTICO: Tabla {TABLE_TOPO_FEATURES} NO EXISTE.\n   ‚Üí Ejecuta primero 01_srtm_processing_v2.py")

topo_features = spark.table(TABLE_TOPO_FEATURES)
count = topo_features.count()

if count == 0:
    raise Exception(f"‚ùå ERROR CR√çTICO: Tabla {TABLE_TOPO_FEATURES} est√° VAC√çA.\n   ‚Üí No hay datos para procesar. Ejecuta 01_srtm_processing_v2.py")

print(f"‚úÖ Features cargados: {count:,} registros")

# Mostrar muestra
print("\nüìã Muestra de features topogr√°ficos:")
display(topo_features.limit(10))

# COMMAND ----------

# MAGIC %md
# MAGIC ## 2. Validar Calidad de Datos GEE

# COMMAND ----------

print("üîç VALIDANDO CALIDAD DE DATOS GEE")
print("=" * 70)

# Verificar que tenemos las columnas de metadatos GEE
gee_columns = ['gee_scale', 'gee_timestamp', 'gee_quality_flag']
missing_columns = [col for col in gee_columns if col not in topo_features.columns]

if missing_columns:
    print(f"‚ö†Ô∏è  ADVERTENCIA: Faltan columnas GEE: {missing_columns}")
    print(f"   Los datos podr√≠an ser de versi√≥n anterior (sin GEE)")
    print(f"   Continuando con validaci√≥n b√°sica...")
    has_gee_metadata = False
else:
    print(f"‚úÖ Todas las columnas GEE presentes")
    has_gee_metadata = True

# Estad√≠sticas de calidad
if has_gee_metadata:
    quality_stats = topo_features.agg(
        F.count("*").alias("total"),
        F.avg("gee_quality_flag").alias("avg_quality"),
        F.min("gee_quality_flag").alias("min_quality"),
        F.sum(F.when(F.col("gee_quality_flag") >= 0.9, 1).otherwise(0)).alias("high_quality_count")
    ).collect()[0]

    print(f"\nüìä Estad√≠sticas de Calidad GEE:")
    print(f"   Total registros: {quality_stats['total']:,}")
    print(f"   Calidad promedio: {quality_stats['avg_quality']:.3f}")
    print(f"   Calidad m√≠nima: {quality_stats['min_quality']:.3f}")
    print(f"   Alta calidad (‚â•0.9): {quality_stats['high_quality_count']:,} ({quality_stats['high_quality_count']/quality_stats['total']*100:.1f}%)")

    if quality_stats['avg_quality'] < 0.8:
        print(f"\n‚ö†Ô∏è  ADVERTENCIA: Calidad promedio baja ({quality_stats['avg_quality']:.3f})")
        print(f"   Considera re-ejecutar 01_srtm_processing_v2.py")

    # Filtrar solo datos de alta calidad para el an√°lisis
    print(f"\nüî¨ Filtrando datos de alta calidad (‚â•0.9)...")
    topo_features_filtered = topo_features.filter(F.col("gee_quality_flag") >= 0.9)
    filtered_count = topo_features_filtered.count()
    print(f"   Registros despu√©s de filtro: {filtered_count:,} ({filtered_count/count*100:.1f}%)")

    if filtered_count < count * 0.5:
        print(f"‚ö†Ô∏è  ADVERTENCIA: Se descart√≥ m√°s del 50% de datos por baja calidad")
else:
    # Si no hay metadatos GEE, usar todos los datos
    topo_features_filtered = topo_features
    filtered_count = count

print("=" * 70)

# COMMAND ----------

# MAGIC %md
# MAGIC ## 3. Calcular Susceptibilidad por Zona y Banda Altitudinal

# COMMAND ----------

print("üéØ Calculando susceptibilidad topogr√°fica por zona y banda altitudinal...")

# Agregar por zona y banda altitudinal
susceptibility_stats = topo_features_filtered.groupBy("zone_name", "elevation_band").agg(
    F.count("*").alias("total_pixels"),
    F.sum(F.when(F.col("is_critical_slope"), 1).otherwise(0)).alias("critical_pixels"),
    F.avg("slope_degrees").alias("avg_slope_degrees"),
    F.avg("elevation").alias("avg_elevation"),
    F.avg("aspect_degrees").alias("avg_aspect_degrees"),
    F.stddev("slope_degrees").alias("stddev_slope"),
    F.min("elevation").alias("min_elevation"),
    F.max("elevation").alias("max_elevation")
)

# Agregar calidad GEE si est√° disponible
if has_gee_metadata:
    susceptibility_stats = susceptibility_stats.join(
        topo_features_filtered.groupBy("zone_name", "elevation_band").agg(
            F.avg("gee_quality_flag").alias("avg_gee_quality"),
            F.avg("gee_scale").alias("avg_gee_scale")
        ),
        on=["zone_name", "elevation_band"],
        how="left"
    )

# Calcular ratio de √°rea cr√≠tica
susceptibility_stats = susceptibility_stats.withColumn(
    "critical_area_ratio",
    F.col("critical_pixels") / F.col("total_pixels")
)

# Calcular susceptibility score (0-1)
# Score mejorado con pesos ajustados:
# - 50% ratio de pendientes cr√≠ticas (factor principal)
# - 30% pendiente promedio normalizada (max 60¬∞)
# - 15% desviaci√≥n est√°ndar (indica variabilidad del terreno)
# - 5% bonus por calidad GEE alta (si disponible)

if has_gee_metadata:
    susceptibility_stats = susceptibility_stats.withColumn(
        "susceptibility_score",
        (
            0.50 * F.col("critical_area_ratio") +
            0.30 * (F.col("avg_slope_degrees") / 60.0) +
            0.15 * (F.when(F.col("stddev_slope").isNull(), 0).otherwise(F.col("stddev_slope")) / 30.0) +
            0.05 * F.col("avg_gee_quality")
        )
    )
else:
    susceptibility_stats = susceptibility_stats.withColumn(
        "susceptibility_score",
        (
            0.55 * F.col("critical_area_ratio") +
            0.30 * (F.col("avg_slope_degrees") / 60.0) +
            0.15 * (F.when(F.col("stddev_slope").isNull(), 0).otherwise(F.col("stddev_slope")) / 30.0)
        )
    )

# Normalizar score a rango 0-1
susceptibility_stats = susceptibility_stats.withColumn(
    "susceptibility_score",
    F.when(F.col("susceptibility_score") > 1.0, 1.0)
     .otherwise(F.col("susceptibility_score"))
)

# Determinar orientaci√≥n dominante
susceptibility_stats = susceptibility_stats.withColumn(
    "dominant_aspect",
    F.when(F.col("avg_aspect_degrees") < 45, "Norte")
     .when(F.col("avg_aspect_degrees") < 135, "Este")
     .when(F.col("avg_aspect_degrees") < 225, "Sur")
     .when(F.col("avg_aspect_degrees") < 315, "Oeste")
     .otherwise("Norte")
)

# Calcular √°rea aproximada
# Si tenemos gee_scale, usar ese valor; sino asumir 30m
if has_gee_metadata:
    # √Årea = (gee_scale)¬≤ por p√≠xel
    susceptibility_stats = susceptibility_stats.withColumn(
        "pixel_area_m2",
        F.col("avg_gee_scale") * F.col("avg_gee_scale")
    )
else:
    # Asumir 30m x 30m = 900 m¬≤ por pixel
    susceptibility_stats = susceptibility_stats.withColumn(
        "pixel_area_m2",
        F.lit(900)
    )

susceptibility_stats = susceptibility_stats.withColumn(
    "total_area_km2",
    (F.col("total_pixels") * F.col("pixel_area_m2")) / 1_000_000  # Convertir a km¬≤
).withColumn(
    "critical_slope_area_km2",
    (F.col("critical_pixels") * F.col("pixel_area_m2")) / 1_000_000
)

# Agregar metadatos
susceptibility_stats = susceptibility_stats.withColumn(
    "zone_id",
    F.concat(
        F.lit("zone_"),
        F.regexp_replace(F.col("zone_name"), " ", "_"),
        F.lit("_"),
        F.regexp_replace(F.col("elevation_band"), " ", "_")
    )
).withColumn(
    "processing_timestamp",
    F.current_timestamp()
).withColumn(
    "data_source",
    F.lit("Google Earth Engine")
)

# Seleccionar columnas finales
if has_gee_metadata:
    susceptibility_final = susceptibility_stats.select(
        "zone_id",
        "zone_name",
        "elevation_band",
        "min_elevation",
        "max_elevation",
        "avg_elevation",
        "total_area_km2",
        "critical_slope_area_km2",
        "susceptibility_score",
        "dominant_aspect",
        "avg_slope_degrees",
        "avg_gee_quality",
        "avg_gee_scale",
        "data_source",
        "processing_timestamp"
    )
else:
    susceptibility_final = susceptibility_stats.select(
        "zone_id",
        "zone_name",
        "elevation_band",
        "min_elevation",
        "max_elevation",
        "avg_elevation",
        "total_area_km2",
        "critical_slope_area_km2",
        "susceptibility_score",
        "dominant_aspect",
        "avg_slope_degrees",
        "data_source",
        "processing_timestamp"
    )

print("‚úÖ Susceptibilidad calculada")

print("\nüìä Top zonas por susceptibilidad:")
display(susceptibility_final.orderBy(F.desc("susceptibility_score")).limit(10))

# COMMAND ----------

# MAGIC %md
# MAGIC ## 4. Validaci√≥n de Resultados de Susceptibilidad

# COMMAND ----------

print("üîç VALIDANDO RESULTADOS DE SUSCEPTIBILIDAD")
print("=" * 70)

# Contar registros
susc_count = susceptibility_final.count()

if susc_count == 0:
    raise Exception("‚ùå ERROR CR√çTICO: No se generaron registros de susceptibilidad.\n   Verifica que la tabla topo_features tenga datos v√°lidos.")

print(f"‚úÖ Registros de susceptibilidad: {susc_count}")

# Verificar rango de scores
score_stats = susceptibility_final.agg(
    F.min("susceptibility_score").alias("min_score"),
    F.max("susceptibility_score").alias("max_score"),
    F.avg("susceptibility_score").alias("avg_score")
).collect()[0]

print(f"\nüìä Estad√≠sticas de Susceptibility Score:")
print(f"   M√≠nimo: {score_stats['min_score']:.3f}")
print(f"   M√°ximo: {score_stats['max_score']:.3f}")
print(f"   Promedio: {score_stats['avg_score']:.3f}")

if score_stats['min_score'] < 0 or score_stats['max_score'] > 1:
    raise Exception(f"‚ùå ERROR: Scores fuera de rango [0,1]: {score_stats['min_score']:.3f} - {score_stats['max_score']:.3f}")

print(f"‚úÖ Scores en rango v√°lido [0,1]")

# Distribuci√≥n de susceptibilidad
print(f"\nüìä Distribuci√≥n de Susceptibilidad:")
susc_distribution = susceptibility_final.withColumn(
    "risk_category",
    F.when(F.col("susceptibility_score") >= 0.8, "Muy Alto")
     .when(F.col("susceptibility_score") >= 0.6, "Alto")
     .when(F.col("susceptibility_score") >= 0.4, "Moderado")
     .when(F.col("susceptibility_score") >= 0.2, "Bajo")
     .otherwise("Muy Bajo")
).groupBy("risk_category").count().orderBy(F.desc("count"))

display(susc_distribution)

print("=" * 70)

# COMMAND ----------

# MAGIC %md
# MAGIC ## 5. Guardar en Tabla Silver: Susceptibility

# COMMAND ----------

print(f"üíæ GUARDANDO SUSCEPTIBILIDAD EN: {TABLE_TOPO_SUSCEPTIBILITY}")
print("=" * 70)

# Verificar que tenemos datos antes de guardar
count_before = susceptibility_final.count()
if count_before == 0:
    raise Exception("‚ùå ERROR CR√çTICO: DataFrame de susceptibilidad est√° VAC√çO. No hay datos para guardar.")

print(f"   Registros a guardar: {count_before}")

susceptibility_final.write \
    .format("delta") \
    .mode("overwrite") \
    .option("overwriteSchema", "true") \
    .saveAsTable(TABLE_TOPO_SUSCEPTIBILITY)

# Verificar que se guard√≥ correctamente
count_after = spark.table(TABLE_TOPO_SUSCEPTIBILITY).count()
if count_after == 0:
    raise Exception(f"‚ùå ERROR CR√çTICO: Tabla {TABLE_TOPO_SUSCEPTIBILITY} est√° VAC√çA despu√©s de guardar")

if count_after != count_before:
    print(f"‚ö†Ô∏è  ADVERTENCIA: Se guardaron {count_after} registros pero se esperaban {count_before}")

print(f"‚úÖ Tabla guardada exitosamente")
print(f"   Tabla: {TABLE_TOPO_SUSCEPTIBILITY}")
print(f"   Registros: {count_after}")
print("=" * 70)

# COMMAND ----------

# MAGIC %md
# MAGIC ## 6. Crear Mapa de Susceptibilidad de Alta Resoluci√≥n (Gold)

# COMMAND ----------

print(f"üó∫Ô∏è  CREANDO MAPA DE SUSCEPTIBILIDAD DE ALTA RESOLUCI√ìN...")
print("=" * 70)

# Unir features con scores de susceptibilidad
susceptibility_map = topo_features_filtered.alias("feat").join(
    susceptibility_final.alias("susc"),
    (F.col("feat.zone_name") == F.col("susc.zone_name")) &
    (F.col("feat.elevation_band") == F.col("susc.elevation_band")),
    "left"
)

# Verificar que el join funcion√≥
map_count = susceptibility_map.count()
if map_count == 0:
    raise Exception("‚ùå ERROR CR√çTICO: Join entre features y susceptibilidad result√≥ vac√≠o")

print(f"‚úÖ Join exitoso: {map_count:,} p√≠xeles")

# Seleccionar columnas para mapa final
susceptibility_map_final = susceptibility_map.select(
    F.concat(
        F.lit("grid_"),
        F.substring(F.col("feat.feature_id"), 5, 12)
    ).alias("grid_id"),
    F.col("feat.zone_name").alias("zone_name"),
    F.col("feat.latitude").alias("latitude"),
    F.col("feat.longitude").alias("longitude"),
    F.col("feat.elevation").alias("elevation"),
    F.col("feat.elevation_band").alias("elevation_band"),
    F.col("feat.slope_degrees").alias("slope_degrees"),
    F.col("feat.aspect_degrees").alias("aspect_degrees"),
    F.col("feat.is_critical_slope").alias("is_critical_slope"),
    F.col("susc.susceptibility_score").alias("susceptibility_score"),
    F.when(F.col("susc.susceptibility_score") >= 0.8, "Muy Alto")
     .when(F.col("susc.susceptibility_score") >= 0.6, "Alto")
     .when(F.col("susc.susceptibility_score") >= 0.4, "Moderado")
     .when(F.col("susc.susceptibility_score") >= 0.2, "Bajo")
     .otherwise("Muy Bajo")
     .alias("risk_category"),
    F.col("susc.data_source").alias("data_source"),
    F.current_timestamp().alias("last_updated")
)

# Si tenemos metadatos GEE, agregarlos
if has_gee_metadata:
    susceptibility_map_final = susceptibility_map.select(
        F.concat(
            F.lit("grid_"),
            F.substring(F.col("feat.feature_id"), 5, 12)
        ).alias("grid_id"),
        F.col("feat.zone_name").alias("zone_name"),
        F.col("feat.latitude").alias("latitude"),
        F.col("feat.longitude").alias("longitude"),
        F.col("feat.elevation").alias("elevation"),
        F.col("feat.elevation_band").alias("elevation_band"),
        F.col("feat.slope_degrees").alias("slope_degrees"),
        F.col("feat.aspect_degrees").alias("aspect_degrees"),
        F.col("feat.is_critical_slope").alias("is_critical_slope"),
        F.col("feat.gee_quality_flag").alias("gee_quality_flag"),
        F.col("susc.susceptibility_score").alias("susceptibility_score"),
        F.when(F.col("susc.susceptibility_score") >= 0.8, "Muy Alto")
         .when(F.col("susc.susceptibility_score") >= 0.6, "Alto")
         .when(F.col("susc.susceptibility_score") >= 0.4, "Moderado")
         .when(F.col("susc.susceptibility_score") >= 0.2, "Bajo")
         .otherwise("Muy Bajo")
         .alias("risk_category"),
        F.col("susc.data_source").alias("data_source"),
        F.current_timestamp().alias("last_updated")
    )

print("‚úÖ Mapa de susceptibilidad creado")

# Mostrar distribuci√≥n de categor√≠as de riesgo
print("\nüìä Distribuci√≥n de Categor√≠as de Riesgo:")
display(
    susceptibility_map_final
    .groupBy("risk_category")
    .agg(F.count("*").alias("pixel_count"))
    .withColumn("percentage", F.round(F.col("pixel_count") / F.sum("pixel_count").over(Window.partitionBy()) * 100, 2))
    .orderBy(F.desc("pixel_count"))
)

print("=" * 70)

# COMMAND ----------

# MAGIC %md
# MAGIC ## 7. Guardar Mapa en Tabla Gold

# COMMAND ----------

print(f"üíæ GUARDANDO MAPA DE SUSCEPTIBILIDAD EN: {TABLE_SUSCEPTIBILITY_MAP}")
print("=" * 70)

# Verificar que tenemos datos antes de guardar
map_count_before = susceptibility_map_final.count()
if map_count_before == 0:
    raise Exception("‚ùå ERROR CR√çTICO: DataFrame del mapa de susceptibilidad est√° VAC√çO. No hay datos para guardar.")

print(f"   Registros a guardar: {map_count_before:,}")

susceptibility_map_final.write \
    .format("delta") \
    .mode("overwrite") \
    .option("overwriteSchema", "true") \
    .saveAsTable(TABLE_SUSCEPTIBILITY_MAP)

# Verificar que se guard√≥ correctamente
map_count_after = spark.table(TABLE_SUSCEPTIBILITY_MAP).count()
if map_count_after == 0:
    raise Exception(f"‚ùå ERROR CR√çTICO: Tabla {TABLE_SUSCEPTIBILITY_MAP} est√° VAC√çA despu√©s de guardar")

if map_count_after != map_count_before:
    print(f"‚ö†Ô∏è  ADVERTENCIA: Se guardaron {map_count_after:,} registros pero se esperaban {map_count_before:,}")

print(f"‚úÖ Mapa guardado exitosamente")
print(f"   Tabla: {TABLE_SUSCEPTIBILITY_MAP}")
print(f"   P√≠xeles: {map_count_after:,}")
print("=" * 70)

# COMMAND ----------

# MAGIC %md
# MAGIC ## 8. Visualizaci√≥n: Heatmap de Susceptibilidad

# COMMAND ----------

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Convertir muestra a Pandas para visualizaci√≥n
sample_size = min(0.1, 10000 / map_count_after)  # Max 10k puntos
map_sample = susceptibility_map_final.sample(sample_size).toPandas()

print(f"üìä Generando visualizaciones (muestra de {len(map_sample):,} puntos)...")

fig, axes = plt.subplots(1, 2, figsize=(18, 7))

# Mapa 1: Scatter plot de susceptibilidad
scatter = axes[0].scatter(
    map_sample['longitude'],
    map_sample['latitude'],
    c=map_sample['susceptibility_score'],
    cmap='YlOrRd',
    s=2,
    alpha=0.6
)
axes[0].set_xlabel('Longitud', fontsize=12)
axes[0].set_ylabel('Latitud', fontsize=12)
axes[0].set_title(f'Mapa de Susceptibilidad - {PILOT_ZONE["name"]} (GEE)', fontsize=14, fontweight='bold')
cbar = plt.colorbar(scatter, ax=axes[0])
cbar.set_label('Score de Susceptibilidad', fontsize=11)
axes[0].grid(True, alpha=0.3)

# Mapa 2: Scatter plot de pendientes cr√≠ticas
critical_only = map_sample[map_sample['is_critical_slope'] == True]
if len(critical_only) > 0:
    axes[1].scatter(
        critical_only['longitude'],
        critical_only['latitude'],
        c='red',
        s=3,
        alpha=0.7,
        label=f'Pendientes {CRITICAL_SLOPE_MIN}-{CRITICAL_SLOPE_MAX}¬∞'
    )
    axes[1].scatter(
        map_sample['longitude'],
        map_sample['latitude'],
        c='lightgray',
        s=1,
        alpha=0.2,
        label='Otras pendientes'
    )
else:
    print("‚ö†Ô∏è  No hay pendientes cr√≠ticas en la muestra")

axes[1].set_xlabel('Longitud', fontsize=12)
axes[1].set_ylabel('Latitud', fontsize=12)
axes[1].set_title('Pendientes Cr√≠ticas (30-45¬∞)', fontsize=14, fontweight='bold')
axes[1].legend()
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print("‚úÖ Visualizaciones generadas")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 9. An√°lisis Estad√≠stico por Banda Altitudinal

# COMMAND ----------

print("üìä AN√ÅLISIS DETALLADO POR BANDA ALTITUDINAL")
print("=" * 80)

susceptibility_analysis = spark.table(TABLE_TOPO_SUSCEPTIBILITY).orderBy(F.desc("susceptibility_score"))

display(susceptibility_analysis)

# COMMAND ----------

# MAGIC %md
# MAGIC ## 10. Identificar Zonas de Mayor Riesgo

# COMMAND ----------

print("üéØ ZONAS DE MAYOR RIESGO TOPOGR√ÅFICO")
print("=" * 80)

high_risk_zones = spark.table(TABLE_TOPO_SUSCEPTIBILITY).filter(
    F.col("susceptibility_score") >= 0.6
).orderBy(F.desc("susceptibility_score"))

high_risk_count = high_risk_zones.count()

if high_risk_count > 0:
    display(high_risk_zones)
    print(f"\n‚ö†Ô∏è  {high_risk_count} zona(s) con susceptibilidad ALTA o MUY ALTA detectadas")
else:
    print("‚úÖ No se detectaron zonas de riesgo ALTO o MUY ALTO")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 11. Exportar Resultados para Visualizaci√≥n

# COMMAND ----------

# Crear resumen para el agente integrador
total_area = float(susceptibility_final.agg(F.sum("total_area_km2")).collect()[0][0])
critical_area = float(susceptibility_final.agg(F.sum("critical_slope_area_km2")).collect()[0][0])
max_susc = float(susceptibility_final.agg(F.max("susceptibility_score")).collect()[0][0])
avg_susc = float(susceptibility_final.agg(F.avg("susceptibility_score")).collect()[0][0])

topo_summary = {
    "agent": "Topografico",
    "version": "v2_gee",
    "data_source": "Google Earth Engine",
    "zone": PILOT_ZONE['name'],
    "total_area_km2": total_area,
    "critical_area_km2": critical_area,
    "critical_area_percentage": (critical_area / total_area * 100) if total_area > 0 else 0,
    "max_susceptibility": max_susc,
    "avg_susceptibility": avg_susc,
    "high_risk_zones": high_risk_count,
    "total_pixels": map_count_after,
    "timestamp": datetime.now().isoformat()
}

if has_gee_metadata:
    avg_quality = float(susceptibility_final.agg(F.avg("avg_gee_quality")).collect()[0][0])
    topo_summary["avg_gee_quality"] = avg_quality

print("\nüìã RESUMEN PARA AGENTE INTEGRADOR:")
print("=" * 80)
for key, value in topo_summary.items():
    if isinstance(value, float):
        print(f"   {key:30s}: {value:.3f}")
    else:
        print(f"   {key:30s}: {value}")
print("=" * 80)

# COMMAND ----------

# MAGIC %md
# MAGIC ## 12. Resumen Final

# COMMAND ----------

print("\n" + "=" * 80)
print("üìä RESUMEN: MAPA DE SUSCEPTIBILIDAD TOPOGR√ÅFICA GEE v2")
print("=" * 80)

print(f"""
üèîÔ∏è  ZONA ANALIZADA: {PILOT_ZONE['name']}

üåç FUENTE DE DATOS:
   ‚Ä¢ Origen: Google Earth Engine
   ‚Ä¢ Dataset: USGS/SRTMGL1_003 (SRTM 30m)
   ‚Ä¢ Procesamiento: Nativo en GEE (ee.Terrain)
   ‚Ä¢ Filtro aplicado: Elevaci√≥n > 2000m

üìä DATOS PROCESADOS:
   ‚Ä¢ Tabla origen: {TABLE_TOPO_FEATURES}
   ‚Ä¢ Registros procesados: {filtered_count:,}""")

if has_gee_metadata:
    print(f"   ‚Ä¢ Calidad GEE promedio: {avg_quality:.3f}")

print(f"""
üíæ OUTPUTS GENERADOS:
   1. {TABLE_TOPO_SUSCEPTIBILITY}
      ‚Üí Susceptibilidad agregada por zona y banda altitudinal
      ‚Üí {susceptibility_final.count()} registros

   2. {TABLE_SUSCEPTIBILITY_MAP}
      ‚Üí Mapa de alta resoluci√≥n (p√≠xel level)
      ‚Üí {map_count_after:,} p√≠xeles

üìà M√âTRICAS CLAVE:
   ‚Ä¢ √Årea total analizada: {total_area:.2f} km¬≤
   ‚Ä¢ √Årea cr√≠tica (30-45¬∞): {critical_area:.2f} km¬≤
   ‚Ä¢ Porcentaje cr√≠tico: {critical_area/total_area*100:.2f}%
   ‚Ä¢ Susceptibilidad m√°xima: {max_susc:.3f}
   ‚Ä¢ Susceptibilidad promedio: {avg_susc:.3f}
   ‚Ä¢ Zonas de alto riesgo: {high_risk_count}

üéØ BANDAS ALTITUDINALES CON MAYOR RIESGO:
""")

high_risk_summary = spark.table(TABLE_TOPO_SUSCEPTIBILITY).orderBy(F.desc("susceptibility_score")).limit(3)
for row in high_risk_summary.collect():
    print(f"   ‚Ä¢ {row.elevation_band:12s}: Score {row.susceptibility_score:.3f} | √Årea cr√≠tica: {row.critical_slope_area_km2:.2f} km¬≤ | Elevaci√≥n: {row.min_elevation:.0f}-{row.max_elevation:.0f}m")

print(f"""
‚ö†Ô∏è  IMPORTANTE:
   ‚Ä¢ Datos 100% de Google Earth Engine
   ‚Ä¢ Sin fallbacks a DEM sint√©tico
   ‚Ä¢ Validaci√≥n estricta aplicada
   ‚Ä¢ Solo datos de alta calidad (‚â•0.9)

‚úÖ AGENTE TOPOGR√ÅFICO v2 COMPLETADO
   ‚Üí Datos listos para Agente Integrador
   ‚Üí Pr√≥ximo paso: Ejecutar Agente 2 (NLP) y Agente 3 (Meteorol√≥gico)
""")

print("=" * 80)
print("‚úÖ PROCESAMIENTO COMPLETADO EXITOSAMENTE")
print("=" * 80)
